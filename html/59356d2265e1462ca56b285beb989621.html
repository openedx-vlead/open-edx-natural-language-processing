<p>The standard N-gram models are trained from some corpus. The finiteness of the training corpus leads to the absence of some perfectly acceptable N-grams. This results in sparse bigram matrices. This method tend to underestimate the probability of strings that do not occur in their training corpus.&nbsp;<br /><br />There are some techniques that can be used for assigning a non-zero probability to these 'zero probability bigrams'. This task of reevaluating some of the zero-probability and low-probability N-grams, and assigning them non-zero values, is called smoothing. Some of the techniques are: Add-One Smoothing, Witten-Bell Discounting, Good-Turing Discounting.<br /><br /></p>
<h4>Add-One Smoothing</h4>
<p><br />In Add-One smoothing, we add one to all the bigram counts before normalizing them into probabilities. This is called add-one smoothing.<br /><br /></p>
<h4>Application on unigrams</h4>
<p><br />The unsmoothed maximum likelihood estimate of the unigram probability can be computed by dividing the count of the word by the total number of word tokens N</p>
<pre>P(w<sub>x</sub>) = c(w<sub>x</sub>)/sum<sub>i</sub>{c(w<sub>i</sub>)} = c(w<sub>x</sub>)/N </pre>
<p><br />Let there be an adjusted count c<sup>*</sup>.<br />c<sub>i</sub><sup>*</sup>&nbsp;= (c&lt;sub&lt;i&lt; sub="" style="color: rgb(68, 68, 68); font-family: "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Verdana, Arial, sans-serif; background-color: rgb(1, 41, 67);"&gt;+1)*N/(N+V)<br />where where V is the total number of word types in the language.&nbsp;<br />Now, probabilities can be calculated by normalizing counts by N.<br />p<sub>i</sub><sup>*</sup>&nbsp;= (c&lt;sub&lt;i&lt; sub=""&gt;+1)/(N+V)&nbsp;<br /><br /></p>
<h4>Application on bigrams</h4>
<p>&lt;sub&lt;i&lt; sub="" style="color: rgb(68, 68, 68); font-family: "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Verdana, Arial, sans-serif; background-color: rgb(1, 41, 67);"&gt;&lt;sub&lt;i&lt; sub=""&gt;<br />Normal bigram probabilities are computed by normalizing each row of counts by the unigram count:<br />P(w<sub>n</sub>|w<sub>n-1</sub>) = C(w<sub>n-1</sub>w<sub>n</sub>)/C(w<sub>n-1</sub>)<br /><br />For add-one smoothed bigram counts we need to augment the unigram count by the number of total word types in the vocabulary V:<br />p<sup>*</sup>(w<sub>n</sub>|w<sub>n-1</sub>) = ( C(w<sub>n-1</sub>w<sub>n</sub>)+1 )/( C(w<sub>n-1</sub>)+V )</p>
<p>Use the full featured <a href="https://htmlg.com">on line HTML editor toolkit</a> to compose web articles!</p>
<p></p>