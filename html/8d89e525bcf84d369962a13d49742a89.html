<div class="content" id="experiment-article-section-2-content">A Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states.In a regular Markov model (Markov Model (Ref:<a href="http://en.wikipedia.org/wiki/Markov_model">http://en.wikipedia.org/wiki/Markov_model</a>)), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters. In a hidden Markov model, the state is not directly visible, but output, dependent on the state, is visible. <br /> <br /><center><img src="http://nlp-iiith.vlabs.ac.in/exp6/Exp4/hmm.jpg" height="300" width="500" /></center><br /> Hidden Markov Model has two important components- <br /> <br /> 1)Transition Probabilities: The one-step transition probability is the probability of transitioning from one state to another in a single step. <br /> <br /> 2)Emission Probabilties: : The output probabilities for an observation from state. Emission probabilities B = { b<sub>i,k</sub> = b<sub>i</sub>(o<sub>k</sub>) = P(o<sub>k</sub> | q<sub>i</sub>) }, where o<sub>k</sub>is an Observation. Informally, B is the probability that the output is o<sub>k</sub> given that the current state is q<sub>i</sub> <br /><br /> For POS tagging, it is assumed that POS are generated as random process, and each process randomly generates a word. Hence, transition matrix denotes the transition probability from one POS to another and emission matrix denotes the probability that a given word can have a particular POS. Word acts as the observations. Some of the basic assumptions are:<br />
<pre>1. First-order (bigram) Markov assumptions:
	a. Limited Horizon: Tag depends only on previous tag
		P(t<sub>i+1</sub> = t<sub>k</sub> | t<sub>1</sub>=t<sub>j1,&acirc;Åš,ti</sub>=t<sub>ji</sub>) = P(t<sub>i+1</sub> = t<sub>k</sub> | t<sub>i</sub> = t<sub>j</sub>)
	b. Time invariance: No change over time
		P(t<sub>i+1</sub> = t<sub>k</sub> | t<sub>i</sub> = t<sub>j</sub>) = P(t<sub>2</sub> = t<sub>k</sub> | t<sub>1</sub> = t<sub>j</sub>) = P(t<sub>j</sub> -&gt; t<sub>k</sub>)
2. Output probabilities:
	Probability of getting word wk for tag t<sub>j</sub>: P(w<sub>k</sub> | t<sub>j</sub>) is independent of other tags or words!<br />
</pre>
<span style="text-decoration: underline;">Calculating the Probabilities </span><br /><br /> Consider the given toy corpus <br />
<pre>EOS/eos<b> They</b>/<i>pronoun</i><b> cut</b>/<i>verb</i><b> the</b>/<i>determiner</i><b> paper</b>/<i>noun</i> EOS/eos <b>He</b>/<i>pronoun</i><b> asked</b>/<i>verb</i><b> for</b>/<i>preposition</i><b> his</b>/<i>pronoun</i><b> cut</b>/<i>noun.</i> EOS/eos<b> Put</b>/<i>verb</i><b> the</b>/<i>determiner</i><b> paper</b>/<i>noun</i><b> in</b>/<i>preposition</i><b> the</b>/<i>determiner</i><b> cut</b>/<i>noun</i> EOS/eos</pre>
<span style="text-decoration: underline;">Calculating Emission Probability Matrix</span><br /><br /> Count the no. of times a specific word occus with a specific POS tag in the corpus. <br /> Here, say for <b>"cut"</b> <br />
<pre>count(cut,verb)=1 
count(cut,noun)=2
count(cut,determiner)=0 </pre>
... and so on zero for other tags too. <br />
<pre>count(cut) = total count of cut = 3 </pre>
Now, calculating the probability <br /> Probability to be filled in the matrix cell at the intersection of <b>cut</b> and <b>verb</b><br />
<pre>P(cut/verb)=count(cut,verb)/count(cut)=1/3=0.33 </pre>
Similarly, <br /> Probability to be filled in the cell at he intersection of <b>cut</b> and <b>determiner</b><br />
<pre>P(cut/determiner)=count(cut,determiner)/count(cut)=0/3=0</pre>
Repeat the same for all the word-tag combination and fill the <br /> <br /> <span style="text-decoration: underline;">Calculating Transition Probability Matrix</span><br /><br /> Count the no. of times a specific tag comes after other POS tags in the corpus. <br /> Here, say for <b>"determiner"</b> <br />
<pre>count(verb,determiner)=2
count(preposition,determiner)=1
count(determiner,determiner)=0 
count(eos,determiner)=0
count(noun,determiner)=0 </pre>
... and so on zero for other tags too. <br />
<pre>count(determiner) = total count of tag 'determiner' = 3 </pre>
Now, calculating the probability <br /> Probability to be filled in the cell at he intersection of <b>determiner</b>(in the column) and <b>verb</b>(in the row)<br />
<pre>P(determiner/verb)=count(verb,determiner)/count(determiner)=2/3=0.66 </pre>
Similarly, <br /> Probability to be filled in the cell at he intersection of <b>determiner</b>(in the column) and <b>noun</b>(in the row)<br />
<pre>P(determiner/noun)=count(noun,determiner)/count(determiner)=0/3=0</pre>
Repeat the same for all the tags <br /> <br /> Note: <b>EOS</b>/<i>eos</i> is a special marker which represents <b>End Of Sentence</b>. <br /><br /></div>
<p></p>