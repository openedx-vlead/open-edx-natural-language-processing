<p>A Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states.In a regular Markov model (Markov Model (Ref: http://en.wikipedia.org/wiki/Markov_model)), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters. In a hidden Markov model, the state is not directly visible, but output, dependent on the state, is visible.&nbsp;<br /><br /></p>
<center><img src="http://cse24-iiith.virtual-labs.ac.in/exp6/Exp4/hmm.jpg" alt="" width="500" height="300" /></center>
<p><br />Hidden Markov Model has two important components-&nbsp;<br /><br />1)Transition Probabilities: The one-step transition probability is the probability of transitioning from one state to another in a single step.&nbsp;<br /><br />2)Emission Probabilties: : The output probabilities for an observation from state. Emission probabilities B = { b<sub>i,k</sub>&nbsp;= b<sub>i</sub>(o<sub>k</sub>) = P(o<sub>k</sub>&nbsp;| q<sub>i</sub>) }, where o<sub>k</sub>is an Observation. Informally, B is the probability that the output is o<sub>k</sub>&nbsp;given that the current state is q<sub>i</sub>&nbsp;<br /><br />For POS tagging, it is assumed that POS are generated as random process, and each process randomly generates a word. Hence, transition matrix denotes the transition probability from one POS to another and emission matrix denotes the probability that a given word can have a particular POS. Word acts as the observations. Some of the basic assumptions are:</p>
<pre>1. First-order (bigram) Markov assumptions: a. Limited Horizon: Tag depends only on previous tag P(t<sub>i+1</sub> = t<sub>k</sub> | t<sub>1</sub>=t<sub>j1,&hellip;,ti</sub>=t<sub>ji</sub>) = P(t<sub>i+1</sub> = t<sub>k</sub> | t<sub>i</sub> = t<sub>j</sub>) b. Time invariance: No change over time P(t<sub>i+1</sub> = t<sub>k</sub> | t<sub>i</sub> = t<sub>j</sub>) = P(t<sub>2</sub> = t<sub>k</sub> | t<sub>1</sub> = t<sub>j</sub>) = P(t<sub>j</sub> -&gt; t<sub>k</sub>) 2. Output probabilities: Probability of getting word wk for tag t<sub>j</sub>: P(w<sub>k</sub> | t<sub>j</sub>) is independent of other tags or words!<br /> </pre>
<p>Calculating the Probabilities&nbsp;<br /><br />Consider the given toy corpus&nbsp;</p>
<pre>EOS/eos<strong> They</strong>/<em>pronoun</em><strong> cut</strong>/<em>verb</em><strong> the</strong>/<em>determiner</em><strong> paper</strong>/<em>noun</em> EOS/eos <strong>He</strong>/<em>pronoun</em><strong> asked</strong>/<em>verb</em><strong>for</strong>/<em>preposition</em><strong> his</strong>/<em>pronoun</em><strong> cut</strong>/<em>noun.</em> EOS/eos<strong> Put</strong>/<em>verb</em><strong> the</strong>/<em>determiner</em><strong> paper</strong>/<em>noun</em><strong>in</strong>/<em>preposition</em><strong> the</strong>/<em>determiner</em><strong> cut</strong>/<em>noun</em> EOS/eos</pre>
<p>Calculating Emission Probability Matrix<br /><br />Count the no. of times a specific word occus with a specific POS tag in the corpus.&nbsp;<br />Here, say for&nbsp;<strong>"cut"</strong>&nbsp;</p>
<pre>count(cut,verb)=1 count(cut,noun)=2 count(cut,determiner)=0</pre>
<p>... and so on zero for other tags too.&nbsp;</p>
<pre>count(cut) = total count of cut = 3</pre>
<p>Now, calculating the probability&nbsp;<br />Probability to be filled in the matrix cell at the intersection of&nbsp;<strong>cut</strong>&nbsp;and&nbsp;<strong>verb</strong></p>
<pre>P(cut/verb)=count(cut,verb)/count(cut)=1/3=0.33</pre>
<p>Similarly,&nbsp;<br />Probability to be filled in the cell at he intersection of&nbsp;<strong>cut</strong>&nbsp;and&nbsp;<strong>determiner</strong></p>
<pre>P(cut/determiner)=count(cut,determiner)/count(cut)=0/3=0</pre>
<p>Repeat the same for all the word-tag combination and fill the&nbsp;<br /><br />Calculating Transition Probability Matrix<br /><br />Count the no. of times a specific tag comes after other POS tags in the corpus.&nbsp;<br />Here, say for&nbsp;<strong>"determiner"</strong>&nbsp;</p>
<pre>count(verb,determiner)=2 count(preposition,determiner)=1 count(determiner,determiner)=0 count(eos,determiner)=0 count(noun,determiner)=0</pre>
<p>... and so on zero for other tags too.&nbsp;</p>
<pre>count(determiner) = total count of tag 'determiner' = 3</pre>
<p>Now, calculating the probability&nbsp;<br />Probability to be filled in the cell at he intersection of&nbsp;<strong>determiner</strong>(in the column) and&nbsp;<strong>verb</strong>(in the row)</p>
<pre>P(determiner/verb)=count(verb,determiner)/count(determiner)=2/3=0.66</pre>
<p>Similarly,&nbsp;<br />Probability to be filled in the cell at he intersection of&nbsp;<strong>determiner</strong>(in the column) and&nbsp;<strong>noun</strong>(in the row)</p>
<pre>P(determiner/noun)=count(noun,determiner)/count(determiner)=0/3=0</pre>
<p>Repeat the same for all the tags&nbsp;<br /><br />Note:&nbsp;<strong>EOS</strong>/<em>eos</em>&nbsp;is a special marker which represents&nbsp;<strong>End Of Sentence</strong>.</p>
<p></p>